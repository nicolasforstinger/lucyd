#!/usr/bin/env python3
"""lucyd-consolidate â€” extract structured memory from sessions and files.

Usage:
    lucyd-consolidate                           # Cron mode: process workspace files
    lucyd-consolidate --backfill                # Backfill from archived sessions
    lucyd-consolidate --maintain                # Maintenance: stale facts, expired commitments
    lucyd-consolidate --evolve                  # Evolve workspace understanding files
    lucyd-consolidate --config ~/lucyd/lucyd.toml
"""

import argparse
import asyncio
import json
import logging
import os
import sqlite3
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

log = logging.getLogger("lucyd-consolidate")


def setup_logging():
    logging.basicConfig(
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        level=logging.INFO,
    )


def parse_args():
    parser = argparse.ArgumentParser(description="Lucyd memory consolidation")
    parser.add_argument("-c", "--config",
                        default=os.environ.get("LUCYD_CONFIG", "./lucyd.toml"),
                        help="Path to lucyd.toml (default: $LUCYD_CONFIG or ./lucyd.toml)")
    parser.add_argument("--backfill", action="store_true",
                        help="Backfill from archived sessions")
    parser.add_argument("--maintain", action="store_true",
                        help="Run maintenance (stale facts, expired commitments)")
    parser.add_argument("--evolve", action="store_true",
                        help="Evolve workspace understanding files (MEMORY.md, USER.md)")
    return parser.parse_args()


def _create_provider(config, model_name: str):
    """Create a provider instance from config."""
    from providers import create_provider
    model_cfg = config.model_config(model_name)
    api_key_env = model_cfg.get("api_key_env", "")
    api_key = os.environ.get(api_key_env, "") if api_key_env else ""
    return create_provider(model_cfg, api_key)


async def run_cron(config, conn):
    """Cron mode: extract facts from workspace markdown files."""
    from consolidation import extract_from_file
    from tools.indexer import scan_workspace

    workspace = config.workspace
    provider = _create_provider(config, config.consolidation_fact_model)

    file_list = scan_workspace(
        workspace,
        include_patterns=config.indexer_include_patterns,
        exclude_dirs=set(config.indexer_exclude_dirs),
    )

    total_facts = 0
    for rel_path, abs_path in file_list:
        try:
            count = await extract_from_file(
                str(abs_path), provider, conn,
                config.consolidation_confidence_threshold,
            )
            if count:
                log.info("Extracted %d facts from %s", count, rel_path)
            total_facts += count
        except Exception:
            log.exception("Failed to process %s", rel_path)

    log.info("Cron complete: %d facts from %d files", total_facts, len(file_list))


async def run_backfill(config, conn):
    """Backfill mode: process archived sessions."""
    from consolidation import consolidate_session
    from context import ContextBuilder

    sessions_dir = config.sessions_dir
    archive = sessions_dir / ".archive"
    if not archive.exists():
        log.info("No archive directory found")
        return

    subagent_provider = _create_provider(config, config.consolidation_fact_model)
    # For backfill, use subagent for episodes too (historical, less persona accuracy)
    primary_provider = subagent_provider

    context_builder = ContextBuilder(
        workspace=config.workspace,
        stable_files=config.context_stable,
        semi_stable_files=config.context_semi_stable,
        tier_overrides=config.context_tiers,
    )

    # Find archived state files
    state_files = sorted(archive.glob("*.state.json"))
    log.info("Found %d archived sessions", len(state_files))

    for state_file in state_files:
        try:
            with open(state_file, encoding="utf-8") as f:
                state = json.load(f)

            session_id = state.get("id", "")
            messages = state.get("messages", [])
            compaction_count = state.get("compaction_count", 0)

            if not session_id or not messages:
                continue

            result = await consolidate_session(
                session_id=session_id,
                messages=messages,
                compaction_count=compaction_count,
                config=config,
                subagent_provider=subagent_provider,
                primary_provider=primary_provider,
                context_builder=context_builder,
                conn=conn,
            )
            if result["facts_added"] or result.get("episode_id"):
                log.info(
                    "Backfill %s: %d facts, episode=%s",
                    session_id[:8], result["facts_added"], result.get("episode_id"),
                )
        except Exception:
            log.exception("Failed to backfill %s", state_file.name)

    log.info("Backfill complete")


def run_maintenance(config, conn):
    """Maintenance mode: stale facts, expired commitments, conflict detection."""
    stale_days = config.maintenance_stale_threshold_days

    # 1. Flag stale facts (log only)
    stale = conn.execute("""
        SELECT id, entity, attribute, value
        FROM facts
        WHERE invalidated_at IS NULL
          AND julianday('now') - julianday(accessed_at) > ?
    """, (stale_days,)).fetchall()
    if stale:
        for s in stale:
            log.info("stale fact: %s.%s = %s", s[1], s[2], s[3])

    # 2. Expire overdue commitments
    expired = conn.execute("""
        UPDATE commitments SET status = 'expired'
        WHERE status = 'open'
          AND deadline IS NOT NULL
          AND deadline < date('now')
    """).rowcount
    if expired:
        log.info("expired %d overdue commitments", expired)

    # 3. Detect conflicts (same entity+attribute, multiple current values)
    conflicts = conn.execute("""
        SELECT f1.entity, f1.attribute,
               f1.value AS val1, f2.value AS val2
        FROM facts f1
        JOIN facts f2 ON f1.entity = f2.entity
            AND f1.attribute = f2.attribute
            AND f1.id < f2.id
        WHERE f1.invalidated_at IS NULL
          AND f2.invalidated_at IS NULL
    """).fetchall()
    for c in conflicts:
        log.warning("conflict: %s.%s = '%s' vs '%s'", c[0], c[1], c[2], c[3])

    # 4. Stats
    stats = {
        "facts": conn.execute(
            "SELECT COUNT(*) FROM facts WHERE invalidated_at IS NULL"
        ).fetchone()[0],
        "episodes": conn.execute(
            "SELECT COUNT(*) FROM episodes"
        ).fetchone()[0],
        "open_commitments": conn.execute(
            "SELECT COUNT(*) FROM commitments WHERE status = 'open'"
        ).fetchone()[0],
        "stale": len(stale),
        "conflicts": len(conflicts),
    }
    log.info("maintenance stats: %s", stats)

    conn.commit()


async def main():
    setup_logging()
    args = parse_args()

    from config import load_config
    from memory_schema import ensure_schema

    config = load_config(args.config)

    conn = sqlite3.connect(config.memory_db, timeout=30)
    conn.execute("PRAGMA journal_mode=WAL")
    conn.row_factory = sqlite3.Row
    ensure_schema(conn)

    try:
        if args.backfill:
            await run_backfill(config, conn)
        elif args.maintain:
            run_maintenance(config, conn)
        elif args.evolve:
            from evolution import run_evolution
            result = await run_evolution(config, conn)
            log.info("Evolution complete: %s", result)
        else:
            await run_cron(config, conn)
    finally:
        conn.close()


if __name__ == "__main__":
    asyncio.run(main())
